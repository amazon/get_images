"""
Author: Sergey Tuchkin, 2022

Test exercise for the interview.

Please write a Python script/application. The script's requirements:

Inputs:
- Mandatory:
  * URL
  * Output directory path
- Optional:
  * Username
  * Password

Outputs:
- Files in the output directory the script downloads based on the extraction rule
explained below

- The script should use Basic Auth when optional username and password specified
- The script should support redirections
- Extraction rule: only the files specified as a source for <img> HTML tag and having .png
(case-insensitive) extension should be downloaded
- Dynamic <img> tags generated by JavaScript can be ignored, i.e., only the original source
code can be evaluated
- The script should be “production-ready”, whatever it means for you
"""

from __future__ import annotations
import argparse
import html.parser
import asyncio
from urllib import parse, request
import pathlib
import logging
import errno
import os
import socket
import hashlib
import base64
import shutil
import re

IMAGE_EXTENSION_LOWERCASE = '.png' # from the description at the top of this script
SOCKET_TIMEOUT_SECONDS = 30 # set socket timeout explicitly, because Python socket library defaults to None
USER_AGENT = "curl/7.68.0" # default useragent of urllib is blacklisted on some sites (e.g. Cloudflare)
MAX_CONCURRENCY = 100 # default limit of 100 simultaneous download requests is same as of aiohttp

class WebPageParser(html.parser.HTMLParser):
    """
    html parser class.

    Attributes:
        img_format: the image files extention in a lowercase (e.g. `.png`)

    Methods:
        parse(data): send data to the parser. Returns tulpe of unique `src` attribute values
    """

    def __init__(self, img_format: str) -> None:
        """
        initialize and reset html parser

        Args:
            img_format: The image files extention in a lowercase (e.g. `.png`)
        """

        super().__init__()
        self.files: set[str] = set() # not a list(), since we don't want to download the same file twice or more
        self.img_format = img_format
        self.reset()

    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
        """Never use this method directly. It is called by parent class' feed() method"""
        for attr_name, attr_value in attrs:
            # extract `src` attributes for each `<img>` tag in the page, filtered by img_format extention
            if tag == "img" and attr_name == "src" and attr_value and parse.urlparse(attr_value).path.lower().endswith(self.img_format):
                self.files.add(attr_value)

    def parse(self, data: str) -> list[str]:
        """send data to the parser. Returns tulpe of unique `src` attribute values"""
        self.feed(data)
        return list(self.files)

class UrlWithAuth:
    """class objects are responsible for all url manipulation (open, download, etc.)"""

    def __init__(self, url: str, userpass: str='') -> None:
        self.url, self.userpass = url, userpass
        self.headers = {'User-agent': USER_AGENT}
        if userpass:
            self.headers['Authorization'] = "Basic " + base64.b64encode(userpass.encode()).decode("ascii")

    @property
    def request(self):
        """returns urllib.request.Request instance"""
        return request.Request(url=self.url, headers=self.headers)

    @property
    def filename(self):
        """returns name of the downloaded file"""
        # set filename to a sha256 digest of the source url to avoid race conditions while
        # saving files with same filename, e.g. /logo.png, but from different sites
        _, extention = os.path.splitext(parse.urlparse(self.url).path)
        m = hashlib.sha256()
        m.update(self.url.encode("ascii"))
        return f"{m.hexdigest()}{extention}"

    def construct_url(self, src_path: str) -> UrlWithAuth:
        """constructs full url from image src attribute"""
        full_url = parse.urljoin(self.url, src_path)
        auth_scope = re.sub(r'/([^/]*)$', '/', self.url) # removes all characters after last forward slash /
        if full_url.startswith(auth_scope):
            return self.__class__(url=full_url, userpass=self.userpass)
        else:
            # do not pass username/password to urls outside of the authentication scope
            return self.__class__(url=full_url)

    def http_get(self) -> str:
        """retrieves content and returns it as a string"""
        with request.urlopen(self.request) as f:
            return f.read().decode()

    def http_download(self, dest: str) -> str|None:
        """retrieves content and saves it to the directory specified by dest argument"""
        path=os.path.join(dest, self.filename)
        logging.info("downloading %s to %s", self.url, path)
        if os.path.exists(path):
            logging.warning("will owerwrite existsing file: %s", path)
        try:
            with request.urlopen(self.request) as resp:
                headers = resp.info()
                size = int(headers.get("Content-Length", "0"))
                with open(path, 'wb') as tfp:
                    shutil.copyfileobj(resp, tfp)
                    bytes_written = tfp.tell() # get last write position 
                    if size and size != bytes_written:
                        logging.warning("file %s might be truncated: %d of %d bytes written", path, bytes_written, size)
        except OSError as err:
            logging.error("downloading %s failed with %s: %s", self.url, type(err).__name__, str(err))
        else:
            return path

    async def http_download_async(self, dest) -> str|None:
        """asyncroneously retrieves content and saves it to the directory specified by dest argument"""
        return await asyncio.get_running_loop().run_in_executor(None, self.http_download, dest)

async def gather_with_concurrency(concurrency: int, *coros):
    """
    limit asyncio concurrency.
    
    See https://stackoverflow.com/a/61478547/4232189 for details
    """
    semaphore = asyncio.Semaphore(concurrency)

    async def sem_coro(coro):
        async with semaphore:
            return await coro

    return await asyncio.gather(*(sem_coro(c) for c in coros))

def parse_html(html: str) -> list[str]:
    """wrapper method to simplify parsing an html page"""
    p = WebPageParser(img_format=IMAGE_EXTENSION_LOWERCASE)
    return p.parse(html)

def parse_args():
    """parse command line arguments"""
    ap = argparse.ArgumentParser()
    ap.add_argument('url', metavar="URL", help='url to retrieve')
    ap.add_argument('dest', metavar="DEST", help='destination folder (output directory)', type=pathlib.Path)
    # The script should use Basic Auth when optional username AND password specified (sic!),
    # but RFC7617 does not prohibit empty usernames or passwords so we leave it up to the user, just like `curl --user` does
    ap.add_argument('--userpass', metavar="USER:PASSWORD", help='username and password, colon-separated', required=False, default='')
    ap.add_argument('--verbose', help='turn on logging', action='store_true')
    return ap.parse_args()

async def main() -> None:
    socket.setdefaulttimeout(SOCKET_TIMEOUT_SECONDS)
    args = parse_args()
    logging.basicConfig(level=logging.INFO)
    logging.getLogger().disabled = not args.verbose
    if not args.dest.is_dir():
        raise FileNotFoundError(errno.ENOENT, 'Output directory does not exist', str(args.dest))
    page_url = UrlWithAuth(url=args.url, userpass=args.userpass)
    page_html = page_url.http_get()
    image_urls = (page_url.construct_url(src) for src in parse_html(page_html))
    download_coros = [url.http_download_async(args.dest) for url in image_urls]
    await gather_with_concurrency(MAX_CONCURRENCY, *download_coros)

if __name__ == "__main__":
    asyncio.run(main())